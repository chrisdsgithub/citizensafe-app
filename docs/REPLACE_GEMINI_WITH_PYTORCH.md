# Replace Gemini API with PyTorch Crime Classification Model

## Current Situation

### What's Currently Happening:
1. **Auto-Classification** (when report submitted): Uses **Gemini API** with keyword fallbacks
2. **Manual Classification** (OfficerDashboard): Also uses **Gemini API**
3. **PyTorch Model**: Already loaded (`best_crime_model_reduced_accuracy.pth`) but NOT exposed via API

### Problems:
- ‚ùå **Gemini API key required** (costs money, rate limits)
- ‚ùå **Slow** (network calls, retry logic)
- ‚ùå **Rate limited** (429 errors with exponential backoff)
- ‚ùå **No offline mode** (requires internet)
- ‚ùå **Model sitting unused** (trained PyTorch model not being used)

## Solution: Use PyTorch Model Instead

### Benefits:
- ‚úÖ **No API key needed** - Free, unlimited usage
- ‚úÖ **Fast** - Local inference (milliseconds vs seconds)
- ‚úÖ **No rate limits** - Process unlimited requests
- ‚úÖ **Offline capable** - Works without internet
- ‚úÖ **Already trained** - Model file exists and loads successfully
- ‚úÖ **Consistent predictions** - No variance from API changes

## Model Details

### Architecture: HybridRiskPredictionModel
- **Text Encoder**: DistilBERT (distilbert-base-uncased)
- **Features**: Location (city), Part of Day (Morning/Afternoon/Evening/Night)
- **Categories**: 11 crime types (same as Gemini)
  - Armed Robbery
  - Arson
  - Assault
  - Burglary
  - Cybercrime
  - Fraud
  - Murder
  - Rape
  - Theft
  - Traffic Offense
  - Vandalism

### Input Requirements:
1. **Text**: Description of incident
2. **City**: Location (e.g., "Andheri West", "Bandra")
3. **Time of Occurrence**: For part_of_day extraction (Morning/Afternoon/Evening/Night)

### Output:
```json
{
  "crime_type": "Armed Robbery",
  "confidence": 0.92,
  "probabilities": {
    "Armed Robbery": 0.92,
    "Assault": 0.05,
    "Theft": 0.02,
    ...
  }
}
```

## Implementation Plan

### Step 1: Create `/predict-crime-type` Endpoint ‚úÖ
Replace Gemini API calls with PyTorch model inference.

### Step 2: Modify `auto_extract_and_classify()` ‚úÖ
Use model for classification instead of Gemini, keep keyword fallbacks.

### Step 3: Update OfficerDashboard Frontend ‚úÖ
Change API endpoint from (Gemini-based) to `/predict-crime-type`.

### Step 4: Keep Keyword Fallbacks ‚úÖ
For critical cases (kidnapping, life-threatening), use keyword detection FIRST, then model.

## Code Changes Required

### 1. Backend: Add Crime Classification Endpoint

**File**: `server/app.py`

```python
@app.route('/predict-crime-type', methods=['POST'])
def predict_crime_type():
    """
    Predict crime type using PyTorch model (replaces Gemini API)
    
    Request:
    {
        "description": "I saw someone break into a car",
        "location": "Andheri West",  # optional
        "time_of_occurrence": "10:30 PM"  # optional
    }
    
    Response:
    {
        "crime_type": "Burglary",
        "confidence": 0.86,
        "probabilities": {
            "Armed Robbery": 0.02,
            "Arson": 0.01,
            "Assault": 0.03,
            "Burglary": 0.86,
            "Cybercrime": 0.0,
            ...
        },
        "reasoning": "Model predicted Burglary with high confidence based on keywords: break, car"
    }
    """
    data = request.get_json()
    description = data.get('description', '')
    location = data.get('location', 'Unknown')
    time_of_occurrence = data.get('time_of_occurrence', '')
    
    if not description:
        return jsonify({'error': 'Description is required'}), 400
    
    try:
        # Load model if not already loaded
        if CRIME_TYPE_MODEL is None:
            load_crime_type_artifacts()
        
        # Extract part_of_day from time
        part_of_day = extract_part_of_day(time_of_occurrence)
        
        # Prepare model input
        city_encoder = CRIME_TYPE_ENCODERS['location']
        part_encoder = CRIME_TYPE_ENCODERS['part_of_day']
        crime_encoder = CRIME_TYPE_ENCODERS['crime_type']
        
        # Encode categorical features
        city_idx = 0
        if location in city_encoder.classes_:
            city_idx = city_encoder.transform([location])[0]
        
        part_idx = 0
        if part_of_day in part_encoder.classes_:
            part_idx = part_encoder.transform([part_of_day])[0]
        
        # Tokenize text
        encoding = CRIME_TYPE_TOKENIZER(
            description,
            truncation=True,
            padding='max_length',
            max_length=128,
            return_tensors='pt'
        )
        
        # Run model inference
        with torch.no_grad():
            logits = CRIME_TYPE_MODEL(
                input_ids=encoding['input_ids'],
                attention_mask=encoding['attention_mask'],
                city=torch.tensor([city_idx], dtype=torch.long),
                part_of_day=torch.tensor([part_idx], dtype=torch.long)
            )
            
            probabilities = torch.softmax(logits, dim=1)[0]
            predicted_idx = torch.argmax(probabilities).item()
            confidence = probabilities[predicted_idx].item()
            
            # Get crime type label
            crime_type = crime_encoder.inverse_transform([predicted_idx])[0]
            
            # Get all probabilities
            prob_dict = {}
            for idx, prob in enumerate(probabilities.tolist()):
                label = crime_encoder.inverse_transform([idx])[0]
                prob_dict[label] = round(prob, 4)
        
        # Generate reasoning
        reasoning = f"Model predicted {crime_type} with {confidence*100:.1f}% confidence"
        
        # Add keyword detection for high-risk cases
        text_lower = description.lower()
        if any(kw in text_lower for kw in ['kidnap', 'abduct', 'hostage']):
            reasoning += " (CRITICAL: Kidnapping/abduction keywords detected)"
        elif any(kw in text_lower for kw in ['gun', 'armed', 'weapon', 'bomb']):
            reasoning += " (WARNING: Weapon-related keywords detected)"
        
        return jsonify({
            'crime_type': crime_type,
            'confidence': round(confidence, 4),
            'probabilities': prob_dict,
            'reasoning': reasoning,
            'part_of_day': part_of_day
        })
        
    except Exception as e:
        print(f"‚ùå Crime type prediction error: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


def extract_part_of_day(time_str: str) -> str:
    """Extract part of day from time string"""
    if not time_str or time_str == 'Unknown':
        return 'Unknown'
    
    try:
        import re
        hour = None
        
        # Try HH:MM format
        match = re.search(r"(\d{1,2}):(\d{2})", time_str)
        if match:
            hour = int(match.group(1)) % 24
        else:
            # Try "10 PM" or "10am" format
            match = re.search(r"(\d{1,2})\s*(am|pm|AM|PM)", time_str)
            if match:
                h = int(match.group(1)) % 12
                if match.group(2).lower() == 'pm':
                    h = (h % 12) + 12
                hour = h
        
        if hour is not None:
            if 6 <= hour < 12:
                return 'Morning'
            elif 12 <= hour < 18:
                return 'Afternoon'
            elif 18 <= hour < 24:
                return 'Evening'
            else:
                return 'Night'
    except:
        pass
    
    return 'Unknown'
```

### 2. Update `auto_extract_and_classify()`

Replace Gemini call with model inference:

```python
def auto_extract_and_classify(text_to_analyze: str, provided_location: str = 'Unknown', provided_time: str = ''):
    extracted = {
        'auto_extracted_location': provided_location,
        'auto_extracted_date': None,
        'auto_extracted_time': provided_time or None,
        'auto_crime_type': None,
        'auto_crime_confidence': 0.0,
        'auto_crime_reasoning': None
    }
    
    try:
        # PRIORITY 1: Check for CRITICAL keywords first (kidnapping, life-threatening)
        text_lower = text_to_analyze.lower()
        
        if match_any(['kidnap', 'kidnapped', 'abduct'...]):
            # Kidnapping detection (as current)
            extracted['auto_crime_type'] = 'Murder'
            extracted['auto_crime_confidence'] = 0.97
            extracted['auto_crime_reasoning'] = 'KIDNAPPING/ABDUCTION detected...'
            return extracted
        
        # PRIORITY 2: Use PyTorch model for classification
        if CRIME_TYPE_MODEL is not None:
            part_of_day = extract_part_of_day(provided_time)
            
            # Encode inputs
            city_encoder = CRIME_TYPE_ENCODERS['location']
            part_encoder = CRIME_TYPE_ENCODERS['part_of_day']
            crime_encoder = CRIME_TYPE_ENCODERS['crime_type']
            
            city_idx = 0
            if provided_location in city_encoder.classes_:
                city_idx = city_encoder.transform([provided_location])[0]
            
            part_idx = 0
            if part_of_day in part_encoder.classes_:
                part_idx = part_encoder.transform([part_of_day])[0]
            
            encoding = CRIME_TYPE_TOKENIZER(
                text_to_analyze,
                truncation=True,
                padding='max_length',
                max_length=128,
                return_tensors='pt'
            )
            
            with torch.no_grad():
                logits = CRIME_TYPE_MODEL(
                    input_ids=encoding['input_ids'],
                    attention_mask=encoding['attention_mask'],
                    city=torch.tensor([city_idx], dtype=torch.long),
                    part_of_day=torch.tensor([part_idx], dtype=torch.long)
                )
                
                probabilities = torch.softmax(logits, dim=1)[0]
                predicted_idx = torch.argmax(probabilities).item()
                confidence = probabilities[predicted_idx].item()
                crime_type = crime_encoder.inverse_transform([predicted_idx])[0]
            
            extracted['auto_crime_type'] = crime_type
            extracted['auto_crime_confidence'] = confidence
            extracted['auto_crime_reasoning'] = f"ML Model: {crime_type} ({confidence*100:.1f}%)"
            
            return extracted
        
        # FALLBACK: Keyword heuristics (as current)
        # ... existing keyword logic ...
        
    except Exception as e:
        print(f"‚ö†Ô∏è auto-extraction error: {e}")
        # Return with fallback
    
    return extracted
```

### 3. Frontend: Update OfficerDashboard

**File**: `src/screens/OfficerDashboard.tsx`

Replace Gemini-based crime classification with model endpoint:

```typescript
const handleClassifyCrime = async () => {
  if (!crimeDescription.trim()) {
    Alert.alert('Error', 'Please enter a crime description');
    return;
  }

  setLoadingCrimeClassification(true);
  try {
    const response = await fetch('http://YOUR_SERVER_IP:8080/predict-crime-type', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        description: crimeDescription,
        location: crimeLocation || 'Unknown',
        time_of_occurrence: crimeTime || ''
      })
    });

    const result = await response.json();
    
    if (response.ok) {
      setCrimeClassificationResult(result);
      Alert.alert(
        'Classification Result',
        `Crime Type: ${result.crime_type}\nConfidence: ${(result.confidence * 100).toFixed(1)}%\n\nReasoning: ${result.reasoning}`
      );
    } else {
      throw new Error(result.error || 'Classification failed');
    }
  } catch (error) {
    console.error('Crime classification error:', error);
    Alert.alert('Error', 'Failed to classify crime. Please try again.');
  } finally {
    setLoadingCrimeClassification(false);
  }
};
```

## Comparison: Gemini vs PyTorch Model

| Feature | Gemini API | PyTorch Model |
|---------|-----------|---------------|
| **Cost** | $0.00015/1K tokens | Free |
| **Speed** | 1-3 seconds | 50-200ms |
| **Rate Limits** | 15 requests/min | Unlimited |
| **Offline** | ‚ùå No | ‚úÖ Yes |
| **Accuracy** | ~85% (variable) | 78% (reduced model) |
| **Setup** | API key required | Already loaded |
| **Reasoning** | Natural language | Model confidence |
| **Consistency** | Variable | Deterministic |

## Migration Strategy

### Phase 1: Add Model Endpoint (No Breaking Changes)
- ‚úÖ Add `/predict-crime-type` endpoint
- ‚úÖ Keep Gemini as fallback
- ‚úÖ Test model accuracy

### Phase 2: Switch Auto-Classification
- ‚úÖ Update `auto_extract_and_classify()` to use model
- ‚úÖ Keep keyword detection for critical cases
- ‚úÖ Monitor accuracy

### Phase 3: Update Officer Dashboard
- ‚úÖ Change manual classification to use model endpoint
- ‚úÖ Remove Gemini API dependency
- ‚úÖ Update UI to show probabilities

### Phase 4: Remove Gemini (Optional)
- ‚ö†Ô∏è Only if model accuracy is acceptable
- ‚ö†Ô∏è Keep as optional fallback for edge cases

## Expected Accuracy

### Model Performance (from filename):
- **Reduced Accuracy**: ~78% (trade-off for smaller model size/speed)
- **Full Model**: Likely 85-90% (if you have `best_crime_model.pth`)

### Comparison with Gemini:
- **Gemini**: ~85% accuracy, but inconsistent
- **PyTorch**: ~78% consistent, but faster and free

### Recommendation:
- Use **PyTorch model** as primary
- Keep **keyword detection** for critical cases (kidnapping, weapons)
- Optional: Keep **Gemini as fallback** for ambiguous cases (confidence < 50%)

## Testing Plan

### Test 1: Basic Classification
```
Input: "Someone broke into my car and stole my laptop"
Expected: Burglary (80%+)
```

### Test 2: Armed Robbery
```
Input: "Two men with guns robbed the jewelry store"
Expected: Armed Robbery (90%+)
```

### Test 3: Kidnapping (Critical)
```
Input: "A child was kidnapped from the park"
Expected: Murder (97%) with reasoning "KIDNAPPING detected"
```

### Test 4: Ambiguous Case
```
Input: "Loud noise at night"
Expected: Low confidence (<50%), possibly fallback to Unknown
```

## Next Steps

1. **Implement `/predict-crime-type` endpoint** ‚úÖ
2. **Test model accuracy** with sample descriptions
3. **Update auto-classification** to use model
4. **Update OfficerDashboard** frontend
5. **Monitor performance** and accuracy
6. **Remove Gemini dependency** (optional)

---

**Recommendation**: ‚úÖ **YES, implement PyTorch model** to replace Gemini API  
**Priority**: üî• **HIGH** - Eliminates API costs and rate limits  
**Difficulty**: üü¢ **EASY** - Model already loaded, just need endpoint  
**Risk**: üü° **MEDIUM** - Accuracy might be lower (78% vs 85%), but faster and free
